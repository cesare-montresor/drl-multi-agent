{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "from shutil import copyfile\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output,display\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym-like environment\n",
    "class Environment():\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        self.env = UnityEnvironment(file_name=path);\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        # self.state_size = self.brain.vector_observation_space_size # bug, returns 8 :.(\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        self.state_size = len(info.vector_observations[0])\n",
    "        self.num_agents = len(info.agents)\n",
    "        self.print_info(info.vector_observations)\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "    \n",
    "    def reset(self,train=True):\n",
    "        info = self.env.reset(train_mode=train)[self.brain_name]\n",
    "        return info.vector_observations\n",
    "    \n",
    "    def step(self,action):\n",
    "        info = self.env.step(action)[self.brain_name]       # send all actions to tne environment\n",
    "        state = info.vector_observations                    # get next state (for each agent)\n",
    "        reward = info.rewards                               # get reward (for each agent)\n",
    "        done = info.local_done   \n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def print_info(self, state=None):\n",
    "        print('Num agents: \\t', self.num_agents)\n",
    "        print('State size: \\t', self.state_size)\n",
    "        print('Action size:\\t', self.action_size)\n",
    "        if state is not None:\n",
    "            print('Raw State shape:\\t', state.shape)\n",
    "            print('Raw State content:\\n', state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num agents: \t 2\n",
      "State size: \t 24\n",
      "Action size:\t 2\n",
      "Raw State shape:\t (2, 24)\n",
      "Raw State content:\n",
      " [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = Environment('Tennis.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        x = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferPrioritized:\n",
    "    def __init__(self, batch_size, buffer_size, seed, min_delta=1e-5):\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.deltas = deque(maxlen=buffer_size)\n",
    "        \n",
    "        self.exp_template = namedtuple(\"exp\", field_names=[\"state\",\"action\",\"reward\",\"new_state\",\"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, new_state, done):\n",
    "        exp = self.exp_template(state, action, reward, new_state, int(done))\n",
    "        self.buffer.append(exp)\n",
    "        self.deltas.append( max(self.deltas) if len(self.deltas) > 0 else self.min_delta )\n",
    "    \n",
    "    def sample(self,priority=0.5):\n",
    "        deltas = np.array(self.deltas)\n",
    "        probs = deltas**priority / np.sum(deltas**priority)\n",
    "        \n",
    "        exp_batch_idx = np.random.choice(np.arange(len(self.buffer)), size=self.batch_size, p=probs, replace=False)\n",
    "         \n",
    "        states = torch.from_numpy(np.vstack([self.buffer[idx].state for idx in exp_batch_idx])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([self.buffer[idx].action for idx in exp_batch_idx])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([self.buffer[idx].reward for idx in exp_batch_idx])).float().to(device)\n",
    "        new_states = torch.from_numpy(np.vstack([self.buffer[idx].new_state for idx in exp_batch_idx])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([self.buffer[idx].done for idx in exp_batch_idx])).float().to(device)\n",
    "        probabilities = torch.from_numpy(np.vstack([probs[idx] for idx in exp_batch_idx])).float().to(device)\n",
    "        \n",
    "        return states, actions, rewards, new_states, dones, probabilities, exp_batch_idx\n",
    "    \n",
    "    def update_deltas(self,idxs,deltas):\n",
    "        for i,idx in enumerate(idxs):\n",
    "            self.deltas[idx] = deltas[i] + self.min_delta\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        #rnd = np.random.randn(*x.shape) \n",
    "        #rnd = np.random.random(self.size)*2-1 \n",
    "        rnd = np.random.standard_normal(self.size)\n",
    "\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * rnd\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, buffer_size, batch_size, learn_every, update_every, random_seed, use_pre):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.use_pre = use_pre\n",
    "        self.num_agents = num_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learn_every = learn_every\n",
    "        self.update_every = update_every\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents,action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        if self.use_pre:\n",
    "            self.memory = ReplayBufferPrioritized(self.buffer_size, self.batch_size, random_seed)\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(self.buffer_size, self.batch_size, random_seed)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, priority=0.0):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for i in range(self.num_agents):\n",
    "            self.memory.add(state[i,:], action[i,:], reward[i], next_state[i,:], done[i])\n",
    "        \n",
    "        self.i_step += 1            \n",
    "        should_learn = (self.i_step % self.learn_every ) == 0\n",
    "        should_update = ( self.i_step % self.update_every ) == 0\n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            if should_learn:\n",
    "                #print('learn',self.i_step)\n",
    "                experiences = None\n",
    "                if self.use_pre:\n",
    "                    experiences = self.memory.sample(priority)\n",
    "                else:\n",
    "                    experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "        \n",
    "            if should_update:\n",
    "                #print('update',self.i_step)\n",
    "                self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "                self.soft_update(self.actor_local, self.actor_target, TAU) \n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        \n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        self.i_step=0\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        if self.use_pre:\n",
    "            states, actions, rewards, next_states, dones, probs, batch_idx = experiences\n",
    "        else:\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "            \n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()    \n",
    "        \n",
    "        if self.use_pre:\n",
    "            Q_error = Q_expected - Q_targets\n",
    "            new_deltas = torch.abs(td_error.detach().squeeze(1)).numpy()\n",
    "            self.memory.update_deltas(batch_idx,new_deltas)\n",
    "            \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128        # minibatch size\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "UPDATE_EVERY = 1        # Update global network ever n-steps\n",
    "LEARN_EVERY  = 1        # Train local network ever n-steps\n",
    "RANDOM_SEED = 0\n",
    "NUM_EPISODES = 4000\n",
    "USE_PRE = False\n",
    "PRE_PRIORITY_START = 0.5\n",
    "PRE_PRIORITY_END = 0.0\n",
    "PRE_PRIORITY_DECAY = 0.991\n",
    "\n",
    "session_name = str(int(time.time()))\n",
    "\n",
    "agent_config = {\n",
    "    'num_agents':env.num_agents,\n",
    "    'state_size':env.state_size,\n",
    "    'action_size':env.action_size,\n",
    "    'buffer_size':BUFFER_SIZE,\n",
    "    'batch_size':BATCH_SIZE,\n",
    "    'learn_every':LEARN_EVERY,\n",
    "    'update_every':UPDATE_EVERY,\n",
    "    'random_seed':RANDOM_SEED,\n",
    "    'use_pre':USE_PRE\n",
    "    \n",
    "}\n",
    "agent = Agent(**agent_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\t Score: -0.00 \tAverage Score: 0.01\n",
      "Episode 200\t Score: -0.00 \tAverage Score: 0.01\n",
      "Episode 300\t Score: 0.10 \tAverage Score: 0.021\n",
      "Episode 400\t Score: 0.05 \tAverage Score: 0.011\n",
      "Episode 500\t Score: -0.00 \tAverage Score: 0.01\n",
      "Episode 600\t Score: 0.05 \tAverage Score: 0.022\n",
      "Episode 700\t Score: 0.05 \tAverage Score: 0.022\n",
      "Episode 800\t Score: -0.00 \tAverage Score: 0.02\n",
      "Episode 900\t Score: 0.05 \tAverage Score: 0.011\n",
      "Episode 1000\t Score: -0.00 \tAverage Score: 0.02\n",
      "Episode 1100\t Score: 0.05 \tAverage Score: 0.011\n",
      "Episode 1200\t Score: -0.00 \tAverage Score: 0.02\n",
      "Episode 1300\t Score: 0.05 \tAverage Score: 0.022\n",
      "Episode 1400\t Score: -0.00 \tAverage Score: 0.02\n",
      "Episode 1500\t Score: -0.00 \tAverage Score: 0.02\n",
      "Episode 1600\t Score: 0.05 \tAverage Score: 0.033\n",
      "Episode 1700\t Score: -0.00 \tAverage Score: 0.03\n",
      "Episode 1800\t Score: 0.05 \tAverage Score: 0.033\n",
      "Episode 1900\t Score: -0.00 \tAverage Score: 0.02\n",
      "Episode 2000\t Score: 0.05 \tAverage Score: 0.033\n",
      "Episode 2100\t Score: -0.00 \tAverage Score: 0.04\n",
      "Episode 2200\t Score: 0.10 \tAverage Score: 0.066\n",
      "Episode 2300\t Score: -0.00 \tAverage Score: 0.08\n",
      "Episode 2400\t Score: 0.10 \tAverage Score: 0.088\n",
      "Episode 2500\t Score: 0.15 \tAverage Score: 0.109\n",
      "Episode 2600\t Score: -0.00 \tAverage Score: 0.12\n",
      "Episode 2700\t Score: 0.05 \tAverage Score: 0.198\n",
      "Episode 2800\t Score: 0.10 \tAverage Score: 0.222\n",
      "Episode 2900\t Score: 0.30 \tAverage Score: 0.222\n",
      "Episode 3000\t Score: 0.25 \tAverage Score: 0.235\n",
      "Episode 3100\t Score: 0.85 \tAverage Score: 0.309\n",
      "Episode 3200\t Score: 0.25 \tAverage Score: 0.324\n",
      "Episode 3300\t Score: 0.35 \tAverage Score: 0.433\n",
      "Episode 3341\t Score: 0.90 \tAverage Score: 0.502\n",
      "Environment solved in 3341 episodes!\tAverage Score: 0.500\n",
      "Episode 3342\t Score: 0.35 \tAverage Score: 0.50\n",
      "Environment solved in 3342 episodes!\tAverage Score: 0.501\n",
      "Episode 3345\t Score: 1.25 \tAverage Score: 0.51\n",
      "Environment solved in 3345 episodes!\tAverage Score: 0.510\n",
      "Episode 3346\t Score: -0.00 \tAverage Score: 0.51\n",
      "Environment solved in 3346 episodes!\tAverage Score: 0.506\n",
      "Episode 3347\t Score: 0.60 \tAverage Score: 0.51\n",
      "Environment solved in 3347 episodes!\tAverage Score: 0.506\n",
      "Episode 3348\t Score: -0.00 \tAverage Score: 0.50\n",
      "Environment solved in 3348 episodes!\tAverage Score: 0.501\n",
      "Episode 3349\t Score: 0.60 \tAverage Score: 0.51\n",
      "Environment solved in 3349 episodes!\tAverage Score: 0.506\n",
      "Episode 3350\t Score: 0.40 \tAverage Score: 0.51\n",
      "Environment solved in 3350 episodes!\tAverage Score: 0.509\n",
      "Episode 3351\t Score: 0.30 \tAverage Score: 0.51\n",
      "Environment solved in 3351 episodes!\tAverage Score: 0.512\n",
      "Episode 3352\t Score: 0.20 \tAverage Score: 0.51\n",
      "Environment solved in 3352 episodes!\tAverage Score: 0.512\n",
      "Episode 3353\t Score: 1.25 \tAverage Score: 0.52\n",
      "Environment solved in 3353 episodes!\tAverage Score: 0.522\n",
      "Episode 3354\t Score: 2.60 \tAverage Score: 0.55\n",
      "Environment solved in 3354 episodes!\tAverage Score: 0.546\n",
      "Episode 3355\t Score: 0.20 \tAverage Score: 0.55\n",
      "Environment solved in 3355 episodes!\tAverage Score: 0.545\n",
      "Episode 3356\t Score: 0.40 \tAverage Score: 0.54\n",
      "Environment solved in 3356 episodes!\tAverage Score: 0.539\n",
      "Episode 3357\t Score: 1.80 \tAverage Score: 0.56\n",
      "Environment solved in 3357 episodes!\tAverage Score: 0.556\n",
      "Episode 3358\t Score: -0.00 \tAverage Score: 0.56\n",
      "Environment solved in 3358 episodes!\tAverage Score: 0.556\n",
      "Episode 3359\t Score: 0.55 \tAverage Score: 0.56\n",
      "Environment solved in 3359 episodes!\tAverage Score: 0.561\n",
      "Episode 3360\t Score: 0.10 \tAverage Score: 0.55\n",
      "Environment solved in 3360 episodes!\tAverage Score: 0.555\n",
      "Episode 3361\t Score: 1.75 \tAverage Score: 0.57\n",
      "Environment solved in 3361 episodes!\tAverage Score: 0.572\n",
      "Episode 3362\t Score: 0.15 \tAverage Score: 0.57\n",
      "Environment solved in 3362 episodes!\tAverage Score: 0.572\n",
      "Episode 3363\t Score: 0.35 \tAverage Score: 0.57\n",
      "Environment solved in 3363 episodes!\tAverage Score: 0.574\n",
      "Episode 3364\t Score: 0.75 \tAverage Score: 0.58\n",
      "Environment solved in 3364 episodes!\tAverage Score: 0.578\n",
      "Episode 3365\t Score: 0.45 \tAverage Score: 0.58\n",
      "Environment solved in 3365 episodes!\tAverage Score: 0.582\n",
      "Episode 3366\t Score: -0.00 \tAverage Score: 0.58\n",
      "Environment solved in 3366 episodes!\tAverage Score: 0.581\n",
      "Episode 3367\t Score: 0.45 \tAverage Score: 0.58\n",
      "Environment solved in 3367 episodes!\tAverage Score: 0.579\n",
      "Episode 3368\t Score: 0.75 \tAverage Score: 0.59\n",
      "Environment solved in 3368 episodes!\tAverage Score: 0.586\n",
      "Episode 3369\t Score: 0.80 \tAverage Score: 0.59\n",
      "Environment solved in 3369 episodes!\tAverage Score: 0.590\n",
      "Episode 3370\t Score: 1.80 \tAverage Score: 0.60\n",
      "Environment solved in 3370 episodes!\tAverage Score: 0.605\n",
      "Episode 3371\t Score: -0.00 \tAverage Score: 0.60\n",
      "Environment solved in 3371 episodes!\tAverage Score: 0.602\n",
      "Episode 3372\t Score: 0.15 \tAverage Score: 0.60\n",
      "Environment solved in 3372 episodes!\tAverage Score: 0.596\n",
      "Episode 3373\t Score: 0.10 \tAverage Score: 0.60\n",
      "Environment solved in 3373 episodes!\tAverage Score: 0.596\n",
      "Episode 3374\t Score: 0.20 \tAverage Score: 0.60\n",
      "Environment solved in 3374 episodes!\tAverage Score: 0.596\n",
      "Episode 3375\t Score: 1.50 \tAverage Score: 0.61\n",
      "Environment solved in 3375 episodes!\tAverage Score: 0.607\n",
      "Episode 3376\t Score: 0.45 \tAverage Score: 0.61\n",
      "Environment solved in 3376 episodes!\tAverage Score: 0.609\n",
      "Episode 3377\t Score: 0.45 \tAverage Score: 0.61\n",
      "Environment solved in 3377 episodes!\tAverage Score: 0.606\n",
      "Episode 3378\t Score: 0.10 \tAverage Score: 0.59\n",
      "Environment solved in 3378 episodes!\tAverage Score: 0.592\n",
      "Episode 3379\t Score: -0.00 \tAverage Score: 0.57\n",
      "Environment solved in 3379 episodes!\tAverage Score: 0.574\n",
      "Episode 3380\t Score: 0.80 \tAverage Score: 0.57\n",
      "Environment solved in 3380 episodes!\tAverage Score: 0.572\n",
      "Episode 3381\t Score: 0.65 \tAverage Score: 0.57\n",
      "Environment solved in 3381 episodes!\tAverage Score: 0.572\n",
      "Episode 3382\t Score: 2.55 \tAverage Score: 0.60\n",
      "Environment solved in 3382 episodes!\tAverage Score: 0.595\n",
      "Episode 3383\t Score: 0.45 \tAverage Score: 0.60\n",
      "Environment solved in 3383 episodes!\tAverage Score: 0.599\n",
      "Episode 3384\t Score: 0.85 \tAverage Score: 0.61\n",
      "Environment solved in 3384 episodes!\tAverage Score: 0.607\n",
      "Episode 3385\t Score: 0.35 \tAverage Score: 0.61\n",
      "Environment solved in 3385 episodes!\tAverage Score: 0.609\n",
      "Episode 3386\t Score: 1.25 \tAverage Score: 0.61\n",
      "Environment solved in 3386 episodes!\tAverage Score: 0.611\n",
      "Episode 3387\t Score: 0.40 \tAverage Score: 0.61\n",
      "Environment solved in 3387 episodes!\tAverage Score: 0.613\n",
      "Episode 3388\t Score: 0.75 \tAverage Score: 0.62\n",
      "Environment solved in 3388 episodes!\tAverage Score: 0.619\n",
      "Episode 3389\t Score: 0.10 \tAverage Score: 0.61\n",
      "Environment solved in 3389 episodes!\tAverage Score: 0.605\n",
      "Episode 3390\t Score: 0.30 \tAverage Score: 0.61\n",
      "Environment solved in 3390 episodes!\tAverage Score: 0.607\n",
      "Episode 3391\t Score: 0.10 \tAverage Score: 0.61\n",
      "Environment solved in 3391 episodes!\tAverage Score: 0.608\n",
      "Episode 3392\t Score: 0.35 \tAverage Score: 0.61\n",
      "Environment solved in 3392 episodes!\tAverage Score: 0.610\n",
      "Episode 3393\t Score: 0.95 \tAverage Score: 0.62\n",
      "Environment solved in 3393 episodes!\tAverage Score: 0.618\n",
      "Episode 3394\t Score: -0.00 \tAverage Score: 0.62\n",
      "Environment solved in 3394 episodes!\tAverage Score: 0.617\n",
      "Episode 3395\t Score: 0.75 \tAverage Score: 0.62\n",
      "Environment solved in 3395 episodes!\tAverage Score: 0.620\n",
      "Episode 3396\t Score: 0.10 \tAverage Score: 0.62\n",
      "Environment solved in 3396 episodes!\tAverage Score: 0.619\n",
      "Episode 3397\t Score: 0.30 \tAverage Score: 0.62\n",
      "Environment solved in 3397 episodes!\tAverage Score: 0.620\n",
      "Episode 3398\t Score: 0.15 \tAverage Score: 0.61\n",
      "Environment solved in 3398 episodes!\tAverage Score: 0.608\n",
      "Episode 3399\t Score: -0.00 \tAverage Score: 0.60\n",
      "Environment solved in 3399 episodes!\tAverage Score: 0.605\n",
      "Episode 3400\t Score: 0.20 \tAverage Score: 0.60\n",
      "\n",
      "Environment solved in 3400 episodes!\tAverage Score: 0.603\n",
      "Episode 3401\t Score: 0.10 \tAverage Score: 0.60\n",
      "Environment solved in 3401 episodes!\tAverage Score: 0.599\n",
      "Episode 3402\t Score: 0.10 \tAverage Score: 0.59\n",
      "Environment solved in 3402 episodes!\tAverage Score: 0.589\n",
      "Episode 3403\t Score: -0.00 \tAverage Score: 0.59\n",
      "Environment solved in 3403 episodes!\tAverage Score: 0.586\n",
      "Episode 3404\t Score: 0.05 \tAverage Score: 0.59\n",
      "Environment solved in 3404 episodes!\tAverage Score: 0.587\n",
      "Episode 3405\t Score: 0.30 \tAverage Score: 0.58\n",
      "Environment solved in 3405 episodes!\tAverage Score: 0.584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3406\t Score: -0.00 \tAverage Score: 0.58\n",
      "Environment solved in 3406 episodes!\tAverage Score: 0.583\n",
      "Episode 3407\t Score: 0.20 \tAverage Score: 0.57\n",
      "Environment solved in 3407 episodes!\tAverage Score: 0.573\n",
      "Episode 3408\t Score: -0.00 \tAverage Score: 0.57\n",
      "Environment solved in 3408 episodes!\tAverage Score: 0.570\n",
      "Episode 3409\t Score: 0.10 \tAverage Score: 0.57\n",
      "Environment solved in 3409 episodes!\tAverage Score: 0.569\n",
      "Episode 3410\t Score: 0.20 \tAverage Score: 0.56\n",
      "Environment solved in 3410 episodes!\tAverage Score: 0.565\n",
      "Episode 3411\t Score: 0.10 \tAverage Score: 0.56\n",
      "Environment solved in 3411 episodes!\tAverage Score: 0.558\n",
      "Episode 3412\t Score: 0.35 \tAverage Score: 0.55\n",
      "Environment solved in 3412 episodes!\tAverage Score: 0.549\n",
      "Episode 3413\t Score: 0.05 \tAverage Score: 0.53\n",
      "Environment solved in 3413 episodes!\tAverage Score: 0.532\n",
      "Episode 3414\t Score: 1.10 \tAverage Score: 0.54\n",
      "Environment solved in 3414 episodes!\tAverage Score: 0.540\n",
      "Episode 3415\t Score: 0.30 \tAverage Score: 0.54\n",
      "Environment solved in 3415 episodes!\tAverage Score: 0.541\n",
      "Episode 3416\t Score: 0.60 \tAverage Score: 0.54\n",
      "Environment solved in 3416 episodes!\tAverage Score: 0.540\n",
      "Episode 3417\t Score: 0.10 \tAverage Score: 0.54\n",
      "Environment solved in 3417 episodes!\tAverage Score: 0.537\n",
      "Episode 3418\t Score: 0.10 \tAverage Score: 0.53\n",
      "Environment solved in 3418 episodes!\tAverage Score: 0.535\n",
      "Episode 3419\t Score: 0.70 \tAverage Score: 0.53\n",
      "Environment solved in 3419 episodes!\tAverage Score: 0.534\n",
      "Episode 3420\t Score: 0.65 \tAverage Score: 0.54\n",
      "Environment solved in 3420 episodes!\tAverage Score: 0.539\n",
      "Episode 3421\t Score: 0.95 \tAverage Score: 0.55\n",
      "Environment solved in 3421 episodes!\tAverage Score: 0.545\n",
      "Episode 3422\t Score: -0.00 \tAverage Score: 0.54\n",
      "Environment solved in 3422 episodes!\tAverage Score: 0.540\n",
      "Episode 3423\t Score: 1.40 \tAverage Score: 0.54\n",
      "Environment solved in 3423 episodes!\tAverage Score: 0.545\n",
      "Episode 3424\t Score: 1.40 \tAverage Score: 0.55\n",
      "Environment solved in 3424 episodes!\tAverage Score: 0.549\n",
      "Episode 3425\t Score: 0.10 \tAverage Score: 0.54\n",
      "Environment solved in 3425 episodes!\tAverage Score: 0.541\n",
      "Episode 3426\t Score: 0.10 \tAverage Score: 0.54\n",
      "Environment solved in 3426 episodes!\tAverage Score: 0.542\n",
      "Episode 3427\t Score: 0.10 \tAverage Score: 0.54\n",
      "Environment solved in 3427 episodes!\tAverage Score: 0.540\n",
      "Episode 3428\t Score: -0.00 \tAverage Score: 0.54\n",
      "Environment solved in 3428 episodes!\tAverage Score: 0.536\n",
      "Episode 3429\t Score: 0.50 \tAverage Score: 0.54\n",
      "Environment solved in 3429 episodes!\tAverage Score: 0.536\n",
      "Episode 3430\t Score: -0.00 \tAverage Score: 0.53\n",
      "Environment solved in 3430 episodes!\tAverage Score: 0.534\n",
      "Episode 3431\t Score: 0.20 \tAverage Score: 0.53\n",
      "Environment solved in 3431 episodes!\tAverage Score: 0.527\n",
      "Episode 3432\t Score: -0.00 \tAverage Score: 0.51\n",
      "Environment solved in 3432 episodes!\tAverage Score: 0.510\n",
      "Episode 3433\t Score: 0.05 \tAverage Score: 0.51\n",
      "Environment solved in 3433 episodes!\tAverage Score: 0.510\n",
      "Episode 3434\t Score: 0.50 \tAverage Score: 0.51\n",
      "Environment solved in 3434 episodes!\tAverage Score: 0.514\n",
      "Episode 3435\t Score: 0.05 \tAverage Score: 0.51\n",
      "Environment solved in 3435 episodes!\tAverage Score: 0.506\n",
      "Episode 3500\t Score: -0.00 \tAverage Score: 0.35\n",
      "Episode 3600\t Score: 0.10 \tAverage Score: 0.334\n",
      "Episode 3700\t Score: 0.15 \tAverage Score: 0.299\n",
      "Episode 3800\t Score: 0.05 \tAverage Score: 0.245\n",
      "Episode 3900\t Score: 1.10 \tAverage Score: 0.265\n",
      "Episode 4000\t Score: 0.10 \tAverage Score: 0.244\n",
      "Episode 4100\t Score: -0.00 \tAverage Score: 0.22\n",
      "Episode 4200\t Score: 0.10 \tAverage Score: 0.311\n",
      "Episode 4300\t Score: -0.00 \tAverage Score: 0.25\n",
      "Episode 4400\t Score: 0.80 \tAverage Score: 0.297\n",
      "Episode 4500\t Score: 0.20 \tAverage Score: 0.299\n",
      "Episode 4600\t Score: 0.05 \tAverage Score: 0.221\n",
      "Episode 4700\t Score: 0.20 \tAverage Score: 0.201\n",
      "Episode 4800\t Score: 0.10 \tAverage Score: 0.221\n",
      "Episode 4900\t Score: 0.50 \tAverage Score: 0.189\n",
      "Episode 5000\t Score: 0.20 \tAverage Score: 0.134\n",
      "Episode 5100\t Score: 0.15 \tAverage Score: 0.166\n",
      "Episode 5200\t Score: -0.00 \tAverage Score: 0.12\n",
      "Episode 5300\t Score: 0.05 \tAverage Score: 0.133\n",
      "Episode 5400\t Score: 0.10 \tAverage Score: 0.187\n",
      "Episode 5500\t Score: -0.00 \tAverage Score: 0.18\n",
      "Episode 5600\t Score: 0.15 \tAverage Score: 0.199\n",
      "Episode 5700\t Score: 0.05 \tAverage Score: 0.200\n",
      "Episode 5800\t Score: 0.30 \tAverage Score: 0.177\n",
      "Episode 5900\t Score: 0.34 \tAverage Score: 0.244\n",
      "Episode 6000\t Score: 0.20 \tAverage Score: 0.188\n",
      "Episode 6100\t Score: 0.10 \tAverage Score: 0.188\n",
      "Episode 6200\t Score: -0.00 \tAverage Score: 0.27\n",
      "Episode 6300\t Score: 0.10 \tAverage Score: 0.244\n",
      "Episode 6400\t Score: 0.25 \tAverage Score: 0.344\n",
      "Episode 6500\t Score: 0.10 \tAverage Score: 0.266\n",
      "Episode 6600\t Score: 0.10 \tAverage Score: 0.256\n",
      "Episode 6700\t Score: 0.10 \tAverage Score: 0.258\n",
      "Episode 6800\t Score: 0.15 \tAverage Score: 0.200\n",
      "Episode 6900\t Score: 0.05 \tAverage Score: 0.166\n",
      "Episode 7000\t Score: 0.15 \tAverage Score: 0.177\n",
      "Episode 7100\t Score: 0.05 \tAverage Score: 0.100\n",
      "Episode 7200\t Score: 0.05 \tAverage Score: 0.133\n",
      "Episode 7300\t Score: 0.15 \tAverage Score: 0.188\n",
      "Episode 7400\t Score: -0.00 \tAverage Score: 0.14\n",
      "Episode 7500\t Score: -0.00 \tAverage Score: 0.13\n",
      "Episode 7600\t Score: 0.15 \tAverage Score: 0.155\n",
      "Episode 7700\t Score: 0.05 \tAverage Score: 0.122\n",
      "Episode 7800\t Score: 0.20 \tAverage Score: 0.111\n",
      "Episode 7900\t Score: -0.00 \tAverage Score: 0.18\n",
      "Episode 8000\t Score: 0.25 \tAverage Score: 0.166\n",
      "Episode 8100\t Score: -0.00 \tAverage Score: 0.17\n",
      "Episode 8200\t Score: 0.10 \tAverage Score: 0.155\n",
      "Episode 8300\t Score: 0.15 \tAverage Score: 0.111\n",
      "Episode 8400\t Score: 0.20 \tAverage Score: 0.207\n",
      "Episode 8500\t Score: -0.00 \tAverage Score: 0.16\n",
      "Episode 8600\t Score: 1.35 \tAverage Score: 0.218\n",
      "Episode 8700\t Score: 0.05 \tAverage Score: 0.190\n",
      "Episode 8800\t Score: 0.05 \tAverage Score: 0.198\n",
      "Episode 8900\t Score: 0.05 \tAverage Score: 0.210\n",
      "Episode 9000\t Score: 0.45 \tAverage Score: 0.244\n",
      "Episode 9100\t Score: 0.30 \tAverage Score: 0.144\n",
      "Episode 9200\t Score: -0.00 \tAverage Score: 0.14\n",
      "Episode 9300\t Score: 0.60 \tAverage Score: 0.188\n",
      "Episode 9400\t Score: 0.45 \tAverage Score: 0.219\n",
      "Episode 9500\t Score: 0.50 \tAverage Score: 0.167\n",
      "Episode 9600\t Score: 0.05 \tAverage Score: 0.223\n",
      "Episode 9700\t Score: 0.10 \tAverage Score: 0.210\n",
      "Episode 9800\t Score: 0.35 \tAverage Score: 0.166\n",
      "Episode 9900\t Score: 0.05 \tAverage Score: 0.212\n",
      "Episode 10000\t Score: 0.05 \tAverage Score: 0.15\n"
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=NUM_EPISODES):\n",
    "    pri = PRE_PRIORITY_START\n",
    "    print_every=100\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        states = env.reset()\n",
    "        agent.reset()\n",
    "        score = np.zeros(env.num_agents)\n",
    "        pri = max(pri*PRE_PRIORITY_DECAY, PRE_PRIORITY_END)\n",
    "        while True:\n",
    "            actions = agent.act(states)                        # select an action (for each agent)\n",
    "            next_states, rewards, dones, _ = env.step(actions)\n",
    "\n",
    "            agent.step(states, actions, rewards, next_states, dones, pri)\n",
    "            score += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            \n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        avg_score_ep = np.mean(score)\n",
    "        avg_score_100 = np.mean(scores_deque)\n",
    "        print('\\rEpisode {}\\t Score: {:.2f} \\tAverage Score: {:.2f}'.format(i_episode,avg_score_ep, avg_score_100 ), end=\"\")\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('') # skip line\n",
    "        \n",
    "        if avg_score_100>=0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode, avg_score_100))\n",
    "            if avg_score_100 > max_score:\n",
    "                max_score = avg_score_100\n",
    "                filename_tpl = 'checkpoints/checkpoint-{}-{}-EP_{}-score_{:.3f}.pth'\n",
    "                filename_tpl_best = 'checkpoints/checkpoint-{}-{}-best.pth'\n",
    "                filename_actor = filename_tpl.format(session_name,'actor', i_episode, avg_score_100)\n",
    "                filename_critic = filename_tpl.format(session_name,'critic', i_episode, avg_score_100)\n",
    "                \n",
    "                torch.save(agent.actor_local.state_dict(), filename_actor)\n",
    "                torch.save(agent.critic_local.state_dict(), filename_critic)\n",
    "                \n",
    "                copyfile(filename_actor, filename_tpl_best.format(session_name,'actor') )\n",
    "                copyfile(filename_critic, filename_tpl_best.format(session_name,'critic') )\n",
    "            break\n",
    "                \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 2.600000038743019\n"
     ]
    }
   ],
   "source": [
    "session_name = '1541008596'\n",
    "filename_tpl_best = 'checkpoints/checkpoint-{}-{}-best.pth'\n",
    "\n",
    "checkpoint_actor = filename_tpl_best.format(session_name,'actor')\n",
    "checkpoint_critic = filename_tpl_best.format(session_name,'critic')\n",
    "\n",
    "agent.actor_local.load_state_dict(torch.load(checkpoint_actor))\n",
    "agent.critic_local.load_state_dict(torch.load(checkpoint_critic))\n",
    "        \n",
    "state = env.reset(train=False)     \n",
    "agent.reset()\n",
    "scores = np.zeros(env.num_agents)                          \n",
    "while True:\n",
    "    action = agent.act(state, add_noise=False)                 \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    scores += reward                         \n",
    "    state = next_state                              \n",
    "    if np.any(done):                              \n",
    "        break\n",
    "print('Total score:',np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
